{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e9b1e847-f794-49f9-9b97-24542bf70a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as st\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1f209f-f32a-4d8d-acaf-38d0a412c277",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "In this exercise we will run through an example of correcting for multiple comparisons with both the Benjamini-Hochberg procedure and the more conservative Bonferroni correction.\n",
    "\n",
    "First, simulate multiple (say, 1000) t-tests comparing two samples with equal means and standard deviations, and save the p-values. Obviously, at p<0.05 we expect that ~5% of the simulations to yield a \"statistically significant\" result (of rejecting the NULL hypothesis that the samples come from distributions with equal means).\n",
    "\n",
    "Second, once you have the simulated p-values, apply both methods to address the multiple comparisons problem.\n",
    "\n",
    "Third, set the sample 1 and sample 2 means to be 1 and 2 respectively, and re-run the exercise. What do you notice? What if you make the difference between means even greater?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3a3d0635-a7dc-4ca5-92b2-313cebe1d793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of tests where p<0.05: 11.5%\n"
     ]
    }
   ],
   "source": [
    "num_tests = 1000\n",
    "n = 10\n",
    "mu1  = 1\n",
    "mu2 = 2\n",
    "std = 3\n",
    "\n",
    "# Generate all samples in one go\n",
    "X1 = np.random.normal(mu1, std, (num_tests, n))\n",
    "X2 = np.random.normal(mu2, std, (num_tests, n))\n",
    "\n",
    "# Apply t-test across the rows (axis=1)\n",
    "tstat, p_vals = st.ttest_ind(X1, X2, axis=1)\n",
    "\n",
    "print(f\"Percent of tests where p<0.05: {(np.sum(p_vals<0.05)/num_tests)*100.}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b9e60720-1378-4afc-a6ae-321eca04a650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted p_value: 5e-05\n",
      "The number of comparisons that remain statistically significant after a Bonferroni Correction: 0\n"
     ]
    }
   ],
   "source": [
    "#Bonferroni \n",
    "new_p = 0.05/1000\n",
    "\n",
    "bon = np.sum(p_vals<new_p) #summing the number of instances where the original p_values are less than the corrected \n",
    "\n",
    "print(f\"Adjusted p_value: {new_p}\")\n",
    "print(f\"The number of comparisons that remain statistically significant after a Bonferroni Correction: {bon}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8e5807-c5d8-47e6-9d62-93a35d91c5d6",
   "metadata": {},
   "source": [
    "## Benjamini–Hochberg procedure\n",
    "Another approach is to more carefully control the false-discovery rate using the Benjamini–Hochberg procedure:\n",
    "\n",
    "1. Rank the individual p-values in ascending order, labeled i=1...n\n",
    "\n",
    "2. For each p-value, calculate its \"critical value\" as (i/n)Q, where i is the rank, n is the total number of tests, and Q is the false discovery rate (a percentage) that you choose (typically 0.05).\n",
    "\n",
    "3. In your rank-ordered, original p-values, find the largest value that is smaller than its associated critical value; this p-value is the new criterion (i.e., reject \n",
    " for all cases for which p ≤ this value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "bd6dd650-a084-4e63-8756-a80d32d76d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot compute a corrected p_value\n"
     ]
    }
   ],
   "source": [
    "# Benjamini–Hochberg\n",
    "\n",
    "Q = 0.05\n",
    "\n",
    "sorted_ps = np.sort(p_vals) #sorting the p_values from smallest to largest \n",
    "\n",
    "ranks = np.arange(1,1001) \n",
    "crit_values = (ranks/1000)*Q\n",
    "\n",
    "indices = np.argwhere((sorted_ps - crit_values)<0)#getting the difference between the p values and critical values. \n",
    "#Any that are <0 are smaller than critical values, so extracting which indices meet that criteria. The last of those should be the \n",
    "#new p_value\n",
    "\n",
    "if np.sum((sorted_ps - crit_values)<0)>0:\n",
    "    ind = indices[-1][0] #index value of new p_value. the [0] is to extract the value because some reason it's a nested array \n",
    "    new_p = sorted_ps[ind]\n",
    "    \n",
    "    bon = np.sum(p_vals<new_p) #summing the number of instances where the original p_values are less than the corrected \n",
    "    \n",
    "    print(f\"Adjusted p_value: {new_p}\")\n",
    "    print(f\"The number of comparisons that remain statistically significant after Benjamini Hochberg: {bon}\")\n",
    "else:\n",
    "    print(\"Cannot compute a corrected p_value\") #if there are no values that reach the criterion for the new p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e354399c-b9a9-4540-b4a2-a5306df6d6a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44adc5bf-c4b0-4b7e-9dd5-585b1b253e96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
